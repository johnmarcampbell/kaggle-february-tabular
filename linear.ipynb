{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv', index_col='id')\n",
    "cat_colz = ['cat'+str(i) for i in range(10)]\n",
    "cont_colz = ['cont'+str(i) for i in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Pipeline Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "metric = 'neg_mean_squared_error'\n",
    "n_jobs = 5\n",
    "\n",
    "transformers = [('one_hot', OneHotEncoder(sparse=False), cat_colz),\n",
    "                ('scaler', StandardScaler(), cont_colz)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Below I test lasso with `alpha` running between 1e-3 and 1. `alpha = 1e-3` scores the best, but going lower doesn't give significantly better performance, the fits take a long time, and we start to run into convergence issues.\n",
    "\n",
    "Best mean CV score is MSE = 0.746 (RMSE = 0.864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes about 40s\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('prep', ColumnTransformer(transformers)),\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "params = {'lasso__alpha': [10**(x/2.) for x in range(-6, 1)]}\n",
    "\n",
    "lasso_search = GridSearchCV(pipeline, params, scoring=metric, n_jobs=n_jobs)\n",
    "_ = lasso_search.fit(data[cat_colz+cont_colz], data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = ['param_lasso__alpha', 'mean_test_score']\n",
    "pd.DataFrame(lasso_search.cv_results_)[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso - Continuous Only\n",
    "As a comparison, I fit lasso with only the continuous features. With only 14 features, the model is very robust against overfitting, and the regularization has little effect. Varying alpha over 15 orders of magnitude from 1e-10 to 1e5 only changes the mean score from 0.7794 to 0.7873 (RMSE from 0.8829 to 0.8873). The categorical features reduce the RMSE by ~1.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes ~20s\n",
    "\n",
    "pipeline = Pipeline([('lasso', Lasso(normalize=True))])\n",
    "\n",
    "params = {'lasso__alpha': [10**x for x in range(-10, 5)]}\n",
    "\n",
    "lasso_cont_search = GridSearchCV(pipeline, params, scoring=metric, n_jobs=n_jobs)\n",
    "_ = lasso_cont_search.fit(data[cont_colz], data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = ['param_lasso__alpha', 'mean_test_score', 'rank_test_score']\n",
    "pd.DataFrame(lasso_cont_search.cv_results_)[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "The Ridge model does ~4e-4 better than the lasso model, a negligable improvement. Although it's performance is more stable over a wide range of `alpha`.\n",
    "\n",
    "Best mean CV score is MSE = 0.746 (RMSE = 0.864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 30s\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('prep', ColumnTransformer(transformers)),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "params = {'ridge__alpha': [10**(x/2.) for x in range(-6, 7)]}\n",
    "\n",
    "ridge_search = GridSearchCV(pipeline, params, scoring=metric, n_jobs=n_jobs)\n",
    "_ = ridge_search.fit(data[cat_colz+cont_colz], data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = ['param_ridge__alpha', 'mean_test_score', 'rank_test_score']\n",
    "pd.DataFrame(ridge_search.cv_results_)[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "\n",
    "ElasticNet performs in between the Ridge and Lasso fits, which is unsurprising. The best fit is for `l1_ratio = 0.01` (almost all Ridge) and `alpha = 0.01`.\n",
    "\n",
    "Best mean CV score is MSE = 0.746 (RMSE = 0.864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes ~8 min\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('prep', ColumnTransformer(transformers)),\n",
    "    ('net', ElasticNet())\n",
    "])\n",
    "\n",
    "params = {'net__alpha': [10**(x/2.) for x in range(-4, 4)],\n",
    "         'net__l1_ratio': [.01, .25, .5, 0.75, 0.99]}\n",
    "\n",
    "elastic_search = GridSearchCV(pipeline, params, scoring=metric, n_jobs=n_jobs)\n",
    "_ = elastic_search.fit(data[cat_colz+cont_colz], data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = ['param_net__alpha', 'param_net__l1_ratio', 'mean_test_score', 'rank_test_score']\n",
    "pd.DataFrame(elastic_search.cv_results_)[display_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
